{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completando PySpark API \n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformaciones\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 26, 55]\n"
     ]
    }
   ],
   "source": [
    "# Map Partitions\n",
    "def avi(l):\n",
    "    return [sum(l)]\n",
    "\n",
    "data = [1, 2, 3, 4, 5,6,7,8,9,10,11,12,13]\n",
    "distData = sc.parallelize(data,3)\n",
    "print distData.mapPartitions(avi).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 7, 10, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "print distData.sample(True,0.5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "[12, 13, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "# \"Union\" e Interseccion\n",
    "data1 = [1, 2, 3, 4, 5,6,7,8,9,10,11,12,13]\n",
    "rdd1 = sc.parallelize(data,3)\n",
    "data2 = [10,11,12,13,14,15,16,17,18]\n",
    "rdd2 = sc.parallelize(data2,3)\n",
    "print rdd1.union(rdd2).collect()\n",
    "print rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 12, 6, 1, 13, 7, 8, 2, 14, 9, 3, 15, 16, 10, 4, 17, 11, 5]\n"
     ]
    }
   ],
   "source": [
    "# Distinct\n",
    "print rdd1.union(rdd2).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 65), ('B', 18), ('C', 19), ('E', 32)]\n"
     ]
    }
   ],
   "source": [
    "# SortByKey\n",
    "data = [('A',65),('C',19),('E',32),('B',18)]\n",
    "distData = sc.parallelize(data,2)\n",
    "print distData.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', <pyspark.resultiterable.ResultIterable object at 0x7f5b1113be90>), ('C', <pyspark.resultiterable.ResultIterable object at 0x7f5b1113bc10>), ('E', <pyspark.resultiterable.ResultIterable object at 0x7f5b1113bcd0>), ('B', <pyspark.resultiterable.ResultIterable object at 0x7f5b1113bb90>)]\n"
     ]
    }
   ],
   "source": [
    "# groupByKey y como convertir un iterable en una lista\n",
    "data = [('A',65),('C',19),('E',32),('A',14),('C',11),('B',18)]\n",
    "distData = sc.parallelize(data,2)\n",
    "print distData.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acciones\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# Count\n",
    "data = [1,21, 2, 3, 4, 5,6,7,8,9,10,11,12,13]\n",
    "distData = sc.parallelize(data,3)\n",
    "print distData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# First\n",
    "print distData.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 4, 10, 21, 3]\n"
     ]
    }
   ],
   "source": [
    "# Take Sample\n",
    "print distData.takeSample(False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# Take Ordered\n",
    "print distData.takeOrdered(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {'A': 2, 'C': 2, 'B': 1, 'E': 1})\n"
     ]
    }
   ],
   "source": [
    "# countByKey\n",
    "data = [('A',65),('C',19),('E',32),('A',14),('C',11),('B',18)]\n",
    "distData = sc.parallelize(data,2)\n",
    "print distData.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joins\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', (65, 'the A')), ('A', (66, 'the A')), ('E', (32, None)), ('C', (19, 'the C')), ('B', (18, None))]\n"
     ]
    }
   ],
   "source": [
    "# joins\n",
    "data1 = [('A',65),('C',19),('E',32),('B',18),('A',66)]\n",
    "distData1 = sc.parallelize(data1,2)\n",
    "data2 = [('A','the A'),('C','the C'),('F','the F'),('F','the real F')]\n",
    "distData2 = sc.parallelize(data2,2)\n",
    "#print distData1.join(distData2).collect()\n",
    "print distData1.leftOuterJoin(distData2).collect()\n",
    "#print distData1.rightOuterJoin(distData2).collect()\n",
    "#print distData1.cartesian(distData2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operaciones con Matrices\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definimos una matriz dispersa de 5x5, cada registro es de tipo (fila,columna,valor) los que no se indican son 0!!\n",
    "# ATENCION: Indexamos a partir de 1\n",
    "mdata = [(1,2,4),(1,5,3),(2,1,3),(3,2,2),(4,4,-1),(5,1,1),(5,5,2)]\n",
    "matrixRDD = sc.parallelize(mdata,2)\n",
    "# Queremos multiplicar la matriz por el vector [1,2,3,4,5]\n",
    "vector = [1,2,3,4,5]\n",
    "m2 = matrixRDD.map(lambda x:(x[0],(x[1],x[2])))\n",
    "print m2.collect()\n",
    "m3 = m2.map(lambda x:(x[0],(vector[x[1][0]-1]*x[1][1])))\n",
    "print m3.collect()\n",
    "m4 = m3.reduceByKey(lambda x,y:(x+y))\n",
    "print m4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
